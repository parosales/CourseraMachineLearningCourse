---
title: "Final Project Report"
author: "Pablo Rosales"
date: "11/22/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Import Libraries
library(caret)
library(sqldf)

# Load data sets
training_raw_df <- read.csv("/Users/prosales/Documents/Personales/08 - Practical Machine Learning/Final Project/Source Data/pml-training.csv")

testing_raw_df <- read.csv("/Users/prosales/Documents/Personales/08 - Practical Machine Learning/Final Project/Source Data/pml-testing.csv")

```


## 1. Understanding the Data:  Exploring and Profiling

Two data sets are provided as part of the project:  a training set and a test set.  
The training set has the following dimensions (rows, columns):
```{r echo = FALSE }
dim(training_raw_df)
```

And the testing set has these dimensions:
```{r echo = FALSE }
dim(testing_raw_df)
```

The purpose of the training set is to provide the basis to construct the model and the purpose of the
testing set is to validate the model and to predict several values for the quiz that is part
of the project.

The variable to predict is named "classe" and has the following values and frequencies in
the training set:

``` {r echo = FALSE}
sqldf('SELECT classe, COUNT(*) FROM training_raw_df GROUP BY classe')
```

There is a great amount of variables that can be used as candidates to predict classe; however
after exploring the data (not shown here), 3 relevant findings can be discovered:

1) Variables "X" and "user_name" are just identifiers that do not seem to influence the classe variable.
2) Several columns do have above 90% of their values set as NA.
3) Several other columns are zero variance predictors.

With these findings, it is possible to discriminate these variables as they do not provide
valuable information to build the model.

## 2. Preparing the Data

As stated in previous section, several variables do not provide enough information according to the
3 criteria.  Hence, these will be removed from the predictors list in 3 stages:  First remove identifiers,
second remove columns with more than 90% of NAs and third, remove columns with zero variance.
After applying the 3 filters, the training set has the following dimensions (rows, cols):


``` {r echo = FALSE}
# Filter 1:  Remove columns X and user_name
training_after_filter_1_df <- within(training_raw_df, rm("X", "user_name") )

# Filter 2:  Remove the features that have less than 10 percent of values (90% NAs)
training_total_rows <- nrow(training_after_filter_1_df)
insuficient_values_threshold <- 0.90 # least allowed percentage of available values
cols_to_remove_after_filter_2 <- c()


for (col in names(training_after_filter_1_df)) {
  if ( 
    (sum(is.na(training_after_filter_1_df[col])) / training_total_rows) > insuficient_values_threshold
  ) {
    cols_to_remove_after_filter_2 <- c(cols_to_remove_after_filter_2, col)  
  }
}


training_after_filter_2_df <- training_after_filter_1_df[, !(names(training_after_filter_1_df) %in% cols_to_remove_after_filter_2)]

# Filter 3:   "predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large"
cols_to_remove_after_filter_3 <- nearZeroVar(training_after_filter_2_df, names = TRUE)

training_after_filter_3_df <- training_after_filter_2_df[, !(names(training_after_filter_2_df) %in% cols_to_remove_after_filter_3)]

dim(training_after_filter_3_df)
```

Having selected the predictors, the next step was to split the testing data set into two subsets:  one for training and the other for
testing the model.  The proportions are:  70% for the training subset and the remaining 30% for the testing subset.

``` {r echo = FALSE}
training_set_indexes <- createDataPartition(y = training_after_filter_3_df$classe, p = 0.70, list = FALSE)

final_training_set <- training_after_filter_3_df[training_set_indexes, ]

final_testing_set <- training_after_filter_3_df[-training_set_indexes, ]
```

## 3. Building and Interpreting the Model

The modeling technique of choice was "Random Forests"; the rationale to choose the modeling technique is:  
1) The nature of the problem is classification.
2) Provide good accuracy.
3) For this domain, interpretability is not an issue.

The resulting model is: 
``` {r echo = FALSE}
rf_fit_1 <- train(classe ~ ., data = final_training_set, method = "rf") 

#rf_fit_1 <- train(classe ~ ., data = final_training_set[sample(dim(final_training_set)[1], size=100), ], method = "rf") 

# visualizing accuracy
rf_fit_1
```

## 4. Predicting with the Model 

To test the model with a different set of values, the test subset was used to predict the classe variable.
The resulting confusion matrix is:

``` {r echo = FALSE}
# predictions
rf_prediction_1 <- predict(rf_fit_1, final_testing_set)

# Prediction vs. Actual values
confusionMatrix(table(rf_prediction_1, final_testing_set$classe))
```

## 5. Interpreting and Evaluating the model

### 5.1. Cross-Validation

In random forests, since it is internally done during the model creation, there is no need to do cross-validation, as it is explained in:  https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr [Ref. 1]

### 5.2. In and Out Sampling Errors

The in sample error is "the error rate you get on the same data set you used to build your predictor" [Ref. 2]; 
in this particular model is: 1 - 0.9981672 = 0.0018328 ~ 0.18328 %

The out of sample error is "the error rate you get on a new data set" [Ref. 2]; 
in this particular model is: 1 - 0.9993 ~ .07%

## 6. Conclusions

1. Random forests predicted with a high accuracy (0.9993) the classe variable.
2. The interpretation of the model is not clear.
3. The model predicted correctly all the values of the quiz. 

## 7. References

[1] https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr

[2] Course slides, lesson # 4.

[3] http://groupware.les.inf.puc-rio.br/har

